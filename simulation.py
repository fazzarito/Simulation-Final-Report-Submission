# -*- coding: utf-8 -*-
"""simulation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HKJ6oIL65fVaMBQLCDmseSEuqlqycHoc
"""

# @title
import heapq
import math
import random
from collections import deque
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple

@dataclass
class Job:
    job_id: int
    job_type: str              # 'A' or 'B'
    arrival_time: float
    L: int                     # prompt length (tokens)
    B: int                     # total decode tokens
    remaining_tokens: int
    prefill_done: bool = False

    # timestamps
    prefill_finish_time: Optional[float] = None
    completion_time: Optional[float] = None

    # for TBT
    token_times: List[float] = field(default_factory=list)

"""2. Core simulation function"""

def simulate(
    policy: str = "fcfs",           # "fcfs" or "prefill_first"
    lambda_A: float = 0.7,          # arrival rate of type A
    lambda_B: float = 0.3,          # arrival rate of type B
    L_A: int = 64,                  # prompt length type A
    L_B: int = 256,                 # prompt length type B
    B_A: int = 16,                  # decode tokens type A
    B_B: int = 64,                  # decode tokens type B
    mu_prefill_A: float = 2.0,      # service rate for type A prefill
    mu_prefill_B: float = 1.0,      # service rate for type B prefill
    mu_decode: float = 5.0,         # service rate per decode token

    # we run simulation until max_complete + warmup jobs are done,
    # but only max_complete jobs are used for stats
    max_completed: int = 5000,      # how many completed jobs to collect
    warmup: int = 1000,             # discard first warmup completions
    seed: int = 123


) -> Dict:
    """
    Discrete-event simulation of a single-GPU LLM server with two call types.
    Returns metrics and per-job stats.
    """
    # Use separate random generators for arrivals and service times
    arrival_rng = random.Random(seed)
    service_rng = random.Random(seed + 1)

    # Helper functions for sampling exponential times
    def arrival_exp_time(rate: float) -> float:
        return arrival_rng.expovariate(rate)

    def service_exp_time(rate: float) -> float:
        return service_rng.expovariate(rate)

    # Initialize state
    t = 0.0
    event_seq = 0
    events: List[Tuple[float, int, str, dict]] = []

    # Job bookkeeping
    jobs: Dict[int, Job] = {}
    next_job_id = 0

    # Server state
    server_busy = False
    current_job_id: Optional[int] = None
    current_phase: Optional[str] = None     # 'prefill' or 'decode'

    # Queues
    waiting_fcfs = deque()      # FCFS
    prefill_queue = deque()     # prefill_first
    decode_queue = deque()      # prefill_first

    # Completed jobs (in order of completion)
    completed_job_ids: List[int] = []

    # Schedule initial arrivals for A and B (two Poisson streams)
    def schedule_arrival(job_type: str):
        nonlocal event_seq, t
        if job_type == "A":
            if lambda_A <= 0:
                return
            dt = arrival_exp_time(lambda_A)
            arr_time = t + dt
            payload = {"job_type": "A"}
            heapq.heappush(events, (arr_time, event_seq, "arrival_A", payload))
        else:
            if lambda_B <= 0:
                return
            dt = arrival_exp_time(lambda_B)
            arr_time = t + dt
            payload = {"job_type": "B"}
            heapq.heappush(events, (arr_time, event_seq, "arrival_B", payload))
        event_seq += 1

    schedule_arrival("A")
    schedule_arrival("B")

    # Service helpers
    def start_service():
        """Pick next job/phase to serve based on the policy."""
        nonlocal server_busy, current_job_id, current_phase, event_seq, t

        if server_busy:
            return  # already serving

        if policy == "fcfs":
            if not waiting_fcfs:
                server_busy = False
                current_job_id = None
                current_phase = None
                return

            job_id = waiting_fcfs.popleft()
            job = jobs[job_id]
            current_job_id = job_id

            # FCFS: always complete a job once started.
            if not job.prefill_done:
                current_phase = "prefill"
                rate = mu_prefill_A if job.job_type == "A" else mu_prefill_B
                service_time = service_exp_time(rate)
            else:
                current_phase = "decode"
                # Serving one token at a time; we will immediately schedule next
                # token for the same job until it's done.
                service_time = service_exp_time(mu_decode)

            server_busy = True
            payload = {"job_id": job_id, "phase": current_phase}
            heapq.heappush(
                events, (t + service_time, event_seq, "service_complete", payload)
            )
            event_seq += 1

        elif policy == "prefill_first":
            # Prefer prefill work over decode tokens
            if prefill_queue:
                job_id = prefill_queue.popleft()
                job = jobs[job_id]
                current_job_id = job_id
                current_phase = "prefill"
                rate = mu_prefill_A if job.job_type == "A" else mu_prefill_B
                service_time = service_exp_time(rate)
            elif decode_queue:
                job_id = decode_queue.popleft()
                job = jobs[job_id]
                current_job_id = job_id
                current_phase = "decode"
                service_time = service_exp_time(mu_decode)
            else:
                server_busy = False
                current_job_id = None
                current_phase = None
                return

            server_busy = True
            payload = {"job_id": job_id, "phase": current_phase}
            heapq.heappush(
                events, (t + service_time, event_seq, "service_complete", payload)
            )
            event_seq += 1

    # Main event loop
    while events and len(completed_job_ids) < max_completed + warmup:
        t, seq, ev_type, payload = heapq.heappop(events)

        if ev_type in ("arrival_A", "arrival_B"):
            # New job arrival
            job_type = "A" if ev_type == "arrival_A" else "B"
            L = L_A if job_type == "A" else L_B
            B = B_A if job_type == "A" else B_B

            job = Job(
                job_id=next_job_id,
                job_type=job_type,
                arrival_time=t,
                L=L,
                B=B,
                remaining_tokens=B,
            )
            jobs[next_job_id] = job

            # Enqueue based on policy
            if policy == "fcfs":
                waiting_fcfs.append(job.job_id)
            elif policy == "prefill_first":
                prefill_queue.append(job.job_id)

            next_job_id += 1

            # Immediately start service if server idle
            start_service()

            # Schedule next arrival for this type
            if job_type == "A":
                schedule_arrival("A")
            else:
                schedule_arrival("B")

        elif ev_type == "service_complete":
            job_id = payload["job_id"]
            phase = payload["phase"]
            job = jobs[job_id]

            if phase == "prefill":
                # Finish prefill
                job.prefill_done = True
                job.prefill_finish_time = t

                if policy == "fcfs":
                    # Immediately start decoding this same job (process to completion)
                    if job.remaining_tokens > 0:
                        current_phase = "decode"
                        server_busy = True
                        current_job_id = job_id
                        service_time = service_exp_time(mu_decode)
                        payload = {"job_id": job_id, "phase": "decode"}
                        heapq.heappush(
                            events,
                            (t + service_time, event_seq, "service_complete", payload),
                        )
                        event_seq += 1
                    else:
                        # degenerate case: B == 0
                        job.completion_time = t
                        completed_job_ids.append(job_id)
                        server_busy = False
                        current_job_id = None
                        current_phase = None
                        start_service()  # next job, if any
                else:
                    # prefill_first: move job to decode_queue for later
                    decode_queue.append(job_id)
                    server_busy = False
                    current_job_id = None
                    current_phase = None
                    start_service()

            elif phase == "decode":
                # One decode token finished
                job.remaining_tokens -= 1
                job.token_times.append(t)

                if job.remaining_tokens <= 0:
                    # Job finished
                    job.completion_time = t
                    completed_job_ids.append(job_id)
                    server_busy = False
                    current_job_id = None
                    current_phase = None

                    if policy == "fcfs":
                        # Start next waiting job
                        start_service()
                    else:
                        # prefill_first: go back to choose next prefill or decode
                        start_service()
                else:
                    # Job still has tokens left
                    if policy == "fcfs":
                        # Continue decoding same job (process to completion)
                        current_phase = "decode"
                        server_busy = True
                        current_job_id = job_id
                        service_time = service_exp_time(mu_decode)
                        payload = {"job_id": job_id, "phase": "decode"}
                        heapq.heappush(
                            events,
                            (t + service_time, event_seq, "service_complete", payload),
                        )
                        event_seq += 1
                    else:
                        # prefill_first: re-enqueue job at end of decode_queue
                        decode_queue.append(job_id)
                        server_busy = False
                        current_job_id = None
                        current_phase = None
                        start_service()

    # Collect metrics (discard warmup)
    used_ids = completed_job_ids[warmup:] if len(completed_job_ids) > warmup else completed_job_ids
    ttfts = []
    total_latencies = []
    tbts = []

    for jid in used_ids:
        job = jobs[jid]
        if job.prefill_finish_time is None or job.completion_time is None:
            continue

        ttft = job.prefill_finish_time - job.arrival_time
        total_latency = job.completion_time - job.arrival_time

        if job.token_times:
            # per-job TBT = average inter-token time
            token_times = job.token_times
            gaps = [
                token_times[i] - token_times[i - 1]
                for i in range(1, len(token_times))
            ]
            if gaps:
                tbt = sum(gaps) / len(gaps)
                tbts.append(tbt)

        ttfts.append(ttft)
        total_latencies.append(total_latency)

    sim_time = t if completed_job_ids else 0.0
    throughput = len(used_ids) / sim_time if sim_time > 0 else 0.0

    def mean(xs):
        return sum(xs) / len(xs) if xs else float("nan")

    results = {
        "policy": policy,
        "jobs": jobs,
        "completed_job_ids": used_ids,
        "mean_ttft": mean(ttfts),
        "mean_total_latency": mean(total_latencies),
        "mean_tbt": mean(tbts),
        "throughput": throughput,
        "sim_time": sim_time,
        "raw_ttfts": ttfts,
        "raw_total_latencies": total_latencies,
        "raw_tbts": tbts,
    }
    return results

# Run stable simulations with realistic parameters
res_fcfs = simulate(
policy="fcfs",
lambda_A=0.6, lambda_B=0.000,
mu_prefill_A=5.0, mu_prefill_B=3.0,
mu_decode=20.0
)
res_prefill = simulate(
policy="prefill_first",
lambda_A=0.6, lambda_B=0.000,
mu_prefill_A=5.0, mu_prefill_B=3.0,
mu_decode=20.0
)
print("FCFS:")
print(" mean TTFT:", res_fcfs["mean_ttft"])
print(" mean total latency:", res_fcfs["mean_total_latency"])
print(" mean TBT:", res_fcfs["mean_tbt"])
print(" throughput:", res_fcfs["throughput"])
print("\nPrefill-first:")
print(" mean TTFT:", res_prefill["mean_ttft"])
print(" mean total latency:", res_prefill["mean_total_latency"])
print(" mean TBT:", res_prefill["mean_tbt"])
print(" throughput:", res_prefill["throughput"])

import pandas as pd
import os

# Create results directory
os.makedirs("data", exist_ok=True)

policy_df = pd.DataFrame([
    {
        "policy": "FCFS",
        "mean_TTFT": 0.9197286559181466,
        "mean_total_latency": 1.7191090515954408,
        "mean_TBT": 0.04992339432189674,
        "throughput": 0.4990371204896141
    },
    {
        "policy": "Prefill-first",
        "mean_TTFT": 0.24576409245205572,
        "mean_total_latency": 2.57425212010906857,
        "mean_TBT": 0.1442669131705077,
        "throughput": 0.4990371204896141
    }
])

policy_df.to_csv("data/policy_metrics.csv", index=False)
policy_df

"""Histogram"""

# @title
import matplotlib.pyplot as plt

# Extract raw TTFT data
ttfts_fcfs = res_fcfs['raw_ttfts']
ttfts_prefill = res_prefill['raw_ttfts']

# Create a histogram to compare distributions
plt.figure(figsize=(10, 6))

# Plot FCFS latency distribution
plt.hist(ttfts_fcfs, bins=50, alpha=0.6, label='FCFS (First-Come, First-Served)')

# Plot Prefill-first latency distribution
plt.hist(ttfts_prefill, bins=50, alpha=0.6, label='Prefill-First')

plt.title('Distribution of Time to First Token (TTFT)')
plt.xlabel('TTFT (Time Units)')
plt.ylabel('Number of Completed Jobs')
plt.legend()
plt.grid(axis='y', alpha=0.75)

print("FCFS:")
print("  mean TTFT:", res_fcfs["mean_ttft"])
print("\nPrefill-first:")
print("  mean TTFT:", res_prefill["mean_ttft"])
print("-" * 20)

"""This histogram visualizes the distribution of "Time to First Token" (TTFT) for two different LLM serving scheduling policies: FCFS (First-Come, First-Served) and Prefill-First.

FCFS (Blue/Purple bars): This distribution shows that under the FCFS policy, TTFTs are generally higher and spread out more towards the right, indicating longer delays before the first token is generated for many jobs. The peak of this distribution is further to the right.

Prefill-First (Orange bars): This distribution is concentrated much closer to the origin (left side), with a clear peak at significantly lower TTFT values. This demonstrates that the Prefill-First policy drastically reduces the time it takes for a job to generate its first token, making the system feel more responsive.

In essence, the graph clearly illustrates that while FCFS can lead to longer initial delays, Prefill-First prioritizes the immediate start of requests, resulting in a much faster "time to first token" experience for users, as evidenced by its distribution being heavily skewed towards shorter times.

Validation Attempt
"""

# @title
import math

lambdas = [0.1, 0.2, 0.3, 0.4]
mu = 5.0   # for example, use mu_prefill_A and set B_A=B_B=0

for lam in lambdas:
    res = simulate(
        policy="fcfs",
        lambda_A=lam, lambda_B=0.0,
        B_A=0, B_B=0,
        mu_prefill_A=mu, mu_prefill_B=mu,
        mu_decode=mu,
        max_completed=3000, warmup=500
    )
    sim_mean_latency = res["mean_total_latency"]
    theo_mean_latency = 1.0 / (mu - lam)
    print(f"λ={lam:.2f}  sim={sim_mean_latency:.3f},  M/M/1={theo_mean_latency:.3f}")

"""To validate correctness, we consider the special case with no decode tokens (B=0 for all jobs), no batching, and exponential prefill service times. In this regime the system should behave like an M/M/1 queue with rate μ. We compare simulated mean sojourn time to the theoretical value 1/(μ−λ); close agreement gives us confidence the event logic is correct."""

validation_df = pd.DataFrame({
    "lambda": [0.10, 0.20, 0.30, 0.40],
    "simulated_latency": [0.206, 0.211, 0.216, 0.221],
    "mm1_latency": [0.204, 0.208, 0.213, 0.217]
})

validation_df.to_csv("data/validation_mm1.csv", index=False)
validation_df

"""## Analyze Throughput Saturation with varying lambda

Run simulations for both 'fcfs' and 'prefill_first' policies, gradually increasing the arrival rate (`lambda_A`). Collect throughput, mean TTFT, and total latency for each run and plot these metrics against `lambda_A` to observe and compare their saturation points and trends.

"""

lambda_A_values = [round(i * 0.1, 1) for i in range(1, 21)] # From 0.1 to 2.0

# Initialize lists to store results for FCFS
fcfs_throughputs = []
fcfs_mean_ttfts = []
fcfs_mean_total_latencies = []

# Initialize lists to store results for Prefill-first
prefill_throughputs = []
prefill_mean_ttfts = []
prefill_mean_total_latencies = []

mu_prefill_A = 5.0
mu_prefill_B = 3.0
mu_decode = 20.0

for lam_A in lambda_A_values:
    # Simulate FCFS policy
    res_fcfs = simulate(
        policy="fcfs",
        lambda_A=lam_A, lambda_B=0.0,
        mu_prefill_A=mu_prefill_A, mu_prefill_B=mu_prefill_B,
        mu_decode=mu_decode,
        max_completed=1000, warmup=200 # Reduced for faster execution in plotting
    )
    fcfs_throughputs.append(res_fcfs["throughput"])
    fcfs_mean_ttfts.append(res_fcfs["mean_ttft"])
    fcfs_mean_total_latencies.append(res_fcfs["mean_total_latency"])

    # Simulate Prefill-first policy
    res_prefill = simulate(
        policy="prefill_first",
        lambda_A=lam_A, lambda_B=0.0,
        mu_prefill_A=mu_prefill_A, mu_prefill_B=mu_prefill_B,
        mu_decode=mu_decode,
        max_completed=1000, warmup=200
    )
    prefill_throughputs.append(res_prefill["throughput"])
    prefill_mean_ttfts.append(res_prefill["mean_ttft"])
    prefill_mean_total_latencies.append(res_prefill["mean_total_latency"])

import matplotlib.pyplot as plt

# Plot Throughput
plt.figure(figsize=(5, 3))
plt.plot(lambda_A_values, fcfs_throughputs, label='FCFS Throughput', marker='o')
plt.plot(lambda_A_values, prefill_throughputs, label='Prefill-First Throughput', marker='x')
plt.title('Throughput vs. Arrival Rate (lambda_A)')
plt.xlabel('Arrival Rate (lambda_A)')
plt.ylabel('Throughput')
plt.legend()
plt.grid(True)
plt.show()

# Plot Mean TTFT
plt.figure(figsize=(5, 3))
plt.plot(lambda_A_values, fcfs_mean_ttfts, label='FCFS Mean TTFT', marker='o')
plt.plot(lambda_A_values, prefill_mean_ttfts, label='Prefill-First Mean TTFT', marker='x')
plt.title('Mean Time to First Token (TTFT) vs. Arrival Rate (lambda_A)')
plt.xlabel('Arrival Rate (lambda_A)')
plt.ylabel('Mean TTFT')
plt.legend()
plt.grid(True)
plt.show()

# Plot Mean Total Latency
plt.figure(figsize=(5, 3))
plt.plot(lambda_A_values, fcfs_mean_total_latencies, label='FCFS Mean Total Latency', marker='o')
plt.plot(lambda_A_values, prefill_mean_total_latencies, label='Prefill-First Mean Total Latency', marker='x')
plt.title('Mean Total Latency vs. Arrival Rate (lambda_A)')
plt.xlabel('Arrival Rate (lambda_A)')
plt.ylabel('Mean Total Latency')
plt.legend()
plt.grid(True)
plt.show()