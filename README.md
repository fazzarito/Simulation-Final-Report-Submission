# ORIE 4580/5580 Final Project — LLM Serving Simulation

This submission contains a discrete-event simulator for a single-GPU LLM-serving system
(prefill + decode) and a reproducible notebook report comparing scheduling policies.

## Folder contents

- `main_report.ipynb` — complete analysis and findings.
- `simulation.py` — demo implementation (imported by the notebook).
- `data/` — optional saved experiment outputs (CSV / pickle) generated by the notebook.
- `README.md` — this file.

## How to run/reproduce results

### Option 1: Run locally with Jupyter
1. Ensure you have Python 3 installed.
2. Install dependencies:
   ```bash
   pip install numpy matplotlib
   ```
   (The notebook uses only standard scientific Python packages)
3. From the project folder, start Jupyter and run the notebook:
   ```bash
   jupyter notebook
   ```
4. Open and run all cells.

### Option 2: Run in Google Colab
1. Upload the entire folder (or zip) to Colab.
2. Open `simulation.py`.
3. Run all cells.

## Reproducibility
- The simulator uses explicit random seeds for arrivals and service times
- The notebook applies a warm-up period (discarding early completions) and then computes
  summary statistics (TTFT, TBT, total latency, throughput)
## Data
- policy_metrics.csv: Summary performance metrics (TTFT, latency, TBT, throughput)
  for FCFS and Prefill-first scheduling policies.
- validation_mm1.csv: Validation of simulated mean latency against M/M/1
  theoretical results for different arrival rates λ.
